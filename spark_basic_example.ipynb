{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark-basic-example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMns+ZbMjjDNWr/D2AgLqY1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gzc/spark/blob/main/spark_basic_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlR3u1M7BlF6"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz  \n",
        "!tar xf /content/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVYch-1qBw-1"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdfopI99Bxar"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THbEgwFK_8T-"
      },
      "source": [
        "# Resilient Distributed Datasets (RDDs)\n",
        "Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fvkljqiAEiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2728e5c-cba6-4a8f-c20a-abd5c759cb85"
      },
      "source": [
        "data = [1, 2, 3, 4, 5]\n",
        "distData = sc.parallelize(data)\n",
        "\n",
        "s = distData.reduce(lambda a, b : a*b)\n",
        "\n",
        "print (s)\n",
        "\n",
        "#.textfile()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1J2MxhvANrv"
      },
      "source": [
        "Once created, the distributed dataset (distData) can be operated on in parallel. For example, we can call distData.reduce(lambda a, b: a + b) to add up the elements of the list. We describe operations on distributed datasets later on.\n",
        "\n",
        "One important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to parallelize (e.g. sc.parallelize(data, 10)). Note: some places in the code use the term slices (a synonym for partitions) to maintain backward compatibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfl5xbQSAZDq"
      },
      "source": [
        "# RDD Operations\n",
        "\n",
        "RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).\n",
        "\n",
        "All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n",
        "\n",
        "By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql_RnV89AtJZ"
      },
      "source": [
        "# Basics\n",
        "\n",
        "To illustrate RDD basics, consider the simple program below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4I6dF_xA0bR",
        "outputId": "b1fd15fc-dfb8-4ef1-c1ae-a4ef4f07b969"
      },
      "source": [
        "def transformFunc(line):\n",
        "  return len(line)\n",
        "\n",
        "def reduceFunc(a, b):\n",
        "  return a+b\n",
        "\n",
        "# Transform: map, filter, FlatMap\n",
        "# Action     reduce\n",
        "\n",
        "#lines = sc.textFile(\"exa.txt\")\n",
        "\n",
        "\n",
        "\n",
        "file = [\"line1\", \"This is line2\"]\n",
        "lines = sc.parallelize(file)\n",
        "print (lines.collect())\n",
        "lineLengths = lines.map(lambda line : len(line))\n",
        "\n",
        "print (lineLengths.collect())\n",
        "totalLength = lineLengths.reduce(reduceFunc)\n",
        "print (totalLength)\n",
        "\n",
        "# transform     map filter flatmap\n",
        "# action        reduce, collect"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['line1', 'This is line2']\n",
            "[5, 13]\n",
            "[5, 13]\n",
            "[5, 13]\n",
            "18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_ukuqCKBJkl"
      },
      "source": [
        "The first line defines a base RDD from an external file. This dataset is not loaded in memory or otherwise acted on: lines is merely a pointer to the file. The second line defines lineLengths as the result of a map transformation. Again, lineLengths is not immediately computed, due to laziness. Finally, we run reduce, which is an action. At this point Spark breaks the computation into tasks to run on separate machines, and each machine runs both its part of the map and a local reduction, returning only its answer to the driver program.\n",
        "\n",
        "If we also wanted to use lineLengths again later, we could add:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7A8nQ8ABLY2"
      },
      "source": [
        "lineLengths.persist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USxCBjjlBNQE"
      },
      "source": [
        "before the reduce, which would cause lineLengths to be saved in memory after the first time it is computed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4r2WROOBb1G"
      },
      "source": [
        "# Passing Functions to Spark\n",
        "Spark’s API relies heavily on passing functions in the driver program to run on the cluster. There are three recommended ways to do this:\n",
        "\n",
        "* Lambda expressions, for simple functions that can be written as an expression. (Lambdas do not support multi-statement functions or statements that do not return a value.)\n",
        "* Local defs inside the function calling into Spark, for longer code.\n",
        "* Top-level functions in a module.\n",
        "\n",
        "For example, to pass a longer function than can be supported using a lambda, consider the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUsNF-c_Bmk9"
      },
      "source": [
        "\"\"\"MyScript.py\"\"\"\n",
        "if __name__ == \"__main__\":\n",
        "    def myFunc(s):\n",
        "        words = s.split(\" \")\n",
        "        return len(words)\n",
        "\n",
        "    sc = SparkContext(...)\n",
        "    sc.textFile(\"file.txt\").map(myFunc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLOMAv2sCQdp"
      },
      "source": [
        "# Working with Key-Value Pairs\n",
        "\n",
        "While most Spark operations work on RDDs containing any type of objects, a few special operations are only available on RDDs of key-value pairs. The most common ones are distributed “shuffle” operations, such as grouping or aggregating the elements by a key.\n",
        "\n",
        "In Python, these operations work on RDDs containing built-in Python tuples such as (1, 2). Simply create such tuples and then call your desired operation.\n",
        "\n",
        "For example, the following code uses the reduceByKey operation on key-value pairs to count how many times each line of text occurs in a file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRFzfpvzCUNM",
        "outputId": "d211a5a2-79a1-4c7d-dc40-3f9efa4ab102"
      },
      "source": [
        "words = [\"hello this is line one\", \"hello this is line two\"]\n",
        "words_rdd = sc.parallelize(words)\n",
        "print (words_rdd.collect())\n",
        "words_rdd = words_rdd.flatMap(lambda line: line.split(\" \"))\n",
        "print (words_rdd.collect())\n",
        "pairs = words_rdd.map(lambda s: (s, 1))\n",
        "print (pairs.collect())\n",
        "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
        "print (counts.collect())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello this is line one', 'hello this is line two']\n",
            "['hello', 'this', 'is', 'line', 'one', 'hello', 'this', 'is', 'line', 'two']\n",
            "[('hello', 1), ('this', 1), ('is', 1), ('line', 1), ('one', 1), ('hello', 1), ('this', 1), ('is', 1), ('line', 1), ('two', 1)]\n",
            "[('this', 2), ('is', 2), ('line', 2), ('two', 1), ('hello', 2), ('one', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1-dPaf6IPCn"
      },
      "source": [
        "#RDD API Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i86pV9EcEDd3"
      },
      "source": [
        "##Word Count\n",
        "In this example, we use a few transformations to build a dataset of (String, Int) pairs called counts and then save it to a file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "nt5VAz2KEPZ9",
        "outputId": "68058e15-ceca-48bc-90bd-cdbe71e9c99e"
      },
      "source": [
        "# text_file = sc.textFile(\"hdfs://...\")\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "text_file = sc.textFile(\"example.txt\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9d50dec5-52f7-430a-af8c-fe35e5367e8a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9d50dec5-52f7-430a-af8c-fe35e5367e8a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving example.txt to example.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Zsoc15VT9HO",
        "outputId": "ceb7fd3c-6e60-4192-c893-3aaf54755058"
      },
      "source": [
        "print (text_file.collect())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Twitter has suspended President Trump from its platform, the company said Friday evening.', '', '\"After close review of recent Tweets from the @realDonaldTrump account and the context around them we have permanently suspended the account due to the risk of further incitement of violence,\" Twitter said.', '\"In the context of horrific events this week, we made it clear on Wednesday that additional violations of the Twitter Rules would potentially result in this very course of action.\"', 'It took an assault on Congress for Facebook and Twitter to draw a line on Trump', 'It took an assault on Congress for Facebook and Twitter to draw a line on Trump', 'Twitter\\'s decision followed two tweets by Trump Friday afternoon that would end up being his last. The tweets violated the company\\'s policy against glorification of violence, Twitter said, and \"these two Tweets must be read in the context of broader events in the country and the ways in which the President\\'s statements can be mobilized by different audiences, including to incite violence, as well as in the context of the pattern of behavior from this account in recent weeks.\"', \"The first tweet was about Trump's supporters.\", '\"The 75,000,000 great American Patriots who voted for me, AMERICA FIRST, and MAKE AMERICA GREAT AGAIN, will have a GIANT VOICE long into the future. They will not be disrespected or treated unfairly in any way, shape or form!!!\"']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv02JCSbGedG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "599f8b86-b7cb-4ae9-916a-383677b4e1d1"
      },
      "source": [
        "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
        "             .map(lambda word: (word, 1)) \\\n",
        "             .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "for x in counts.collect():\n",
        "  print (x)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Twitter', 6)\n",
            "('suspended', 2)\n",
            "('President', 1)\n",
            "('Trump', 4)\n",
            "('platform,', 1)\n",
            "('said', 1)\n",
            "('Friday', 2)\n",
            "('evening.', 1)\n",
            "('', 1)\n",
            "('\"After', 1)\n",
            "('close', 1)\n",
            "('of', 10)\n",
            "('account', 3)\n",
            "('around', 1)\n",
            "('them', 1)\n",
            "('we', 2)\n",
            "('have', 2)\n",
            "('permanently', 1)\n",
            "('due', 1)\n",
            "('risk', 1)\n",
            "('further', 1)\n",
            "('incitement', 1)\n",
            "('violence,\"', 1)\n",
            "('\"In', 1)\n",
            "('horrific', 1)\n",
            "('this', 3)\n",
            "('clear', 1)\n",
            "('additional', 1)\n",
            "('violations', 1)\n",
            "('Rules', 1)\n",
            "('would', 2)\n",
            "('potentially', 1)\n",
            "('result', 1)\n",
            "('in', 7)\n",
            "('very', 1)\n",
            "('It', 2)\n",
            "('took', 2)\n",
            "('an', 2)\n",
            "('Congress', 2)\n",
            "('Facebook', 2)\n",
            "('draw', 2)\n",
            "('line', 2)\n",
            "(\"Twitter's\", 1)\n",
            "('decision', 1)\n",
            "('two', 2)\n",
            "('afternoon', 1)\n",
            "('end', 1)\n",
            "('his', 1)\n",
            "('The', 2)\n",
            "('violated', 1)\n",
            "(\"company's\", 1)\n",
            "('policy', 1)\n",
            "('against', 1)\n",
            "('glorification', 1)\n",
            "('violence,', 2)\n",
            "('must', 1)\n",
            "('read', 1)\n",
            "('broader', 1)\n",
            "('country', 1)\n",
            "('mobilized', 1)\n",
            "('different', 1)\n",
            "('incite', 1)\n",
            "('as', 2)\n",
            "('pattern', 1)\n",
            "('behavior', 1)\n",
            "('weeks.\"', 1)\n",
            "('was', 1)\n",
            "('supporters.', 1)\n",
            "('American', 1)\n",
            "('voted', 1)\n",
            "('AMERICA', 2)\n",
            "('MAKE', 1)\n",
            "('AGAIN,', 1)\n",
            "('VOICE', 1)\n",
            "('long', 1)\n",
            "('into', 1)\n",
            "('They', 1)\n",
            "('disrespected', 1)\n",
            "('treated', 1)\n",
            "('shape', 1)\n",
            "('form!!!\"', 1)\n",
            "('has', 1)\n",
            "('from', 3)\n",
            "('its', 1)\n",
            "('the', 15)\n",
            "('company', 1)\n",
            "('review', 1)\n",
            "('recent', 2)\n",
            "('Tweets', 2)\n",
            "('@realDonaldTrump', 1)\n",
            "('and', 6)\n",
            "('context', 4)\n",
            "('to', 4)\n",
            "('said.', 1)\n",
            "('events', 2)\n",
            "('week,', 1)\n",
            "('made', 1)\n",
            "('it', 1)\n",
            "('on', 5)\n",
            "('Wednesday', 1)\n",
            "('that', 2)\n",
            "('course', 1)\n",
            "('action.\"', 1)\n",
            "('assault', 2)\n",
            "('for', 3)\n",
            "('a', 3)\n",
            "('followed', 1)\n",
            "('tweets', 2)\n",
            "('by', 2)\n",
            "('up', 1)\n",
            "('being', 1)\n",
            "('last.', 1)\n",
            "('said,', 1)\n",
            "('\"these', 1)\n",
            "('be', 3)\n",
            "('ways', 1)\n",
            "('which', 1)\n",
            "(\"President's\", 1)\n",
            "('statements', 1)\n",
            "('can', 1)\n",
            "('audiences,', 1)\n",
            "('including', 1)\n",
            "('well', 1)\n",
            "('first', 1)\n",
            "('tweet', 1)\n",
            "('about', 1)\n",
            "(\"Trump's\", 1)\n",
            "('\"The', 1)\n",
            "('75,000,000', 1)\n",
            "('great', 1)\n",
            "('Patriots', 1)\n",
            "('who', 1)\n",
            "('me,', 1)\n",
            "('FIRST,', 1)\n",
            "('GREAT', 1)\n",
            "('will', 2)\n",
            "('GIANT', 1)\n",
            "('future.', 1)\n",
            "('not', 1)\n",
            "('or', 2)\n",
            "('unfairly', 1)\n",
            "('any', 1)\n",
            "('way,', 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zebYKFfcEEln"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG55CEl0DKkV"
      },
      "source": [
        "## Pi Estimation\n",
        "Spark can also be used for compute-intensive tasks. This code estimates π by \"throwing darts\" at a circle. We pick random points in the unit square ((0, 0) to (1,1)) and see how many fall in the unit circle. The fraction should be π / 4, so we use this to get our estimate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSU3OlbSDOtg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2596e318-b09d-4b81-bac2-ae4b88e9c059"
      },
      "source": [
        "import random\n",
        "\n",
        "def inside(p):\n",
        "    x, y = random.random(), random.random()\n",
        "    return x*x + y*y < 1\n",
        "\n",
        "NUM_SAMPLES = 10000000\n",
        "count = sc.parallelize(range(0, NUM_SAMPLES)) \\\n",
        "             .filter(inside).count()\n",
        "print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pi is roughly 3.141481\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpgWiAV6IF5k"
      },
      "source": [
        "#DataFrame API Examples\n",
        "In Spark, a DataFrame is a distributed collection of data organized into named columns. Users can use DataFrame API to perform various relational operations on both external data sources and Spark’s built-in distributed collections without providing specific procedures for processing data. Also, programs based on DataFrame API will be automatically optimized by Spark’s built-in optimizer, Catalyst."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA50JLUHIYNv"
      },
      "source": [
        "##Text Search\n",
        "In this example, we search through the error messages in a log file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hbez2SbvNINM"
      },
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZJYpYBgIdvT",
        "outputId": "fad75cac-e35f-4931-8217-833cb5701147"
      },
      "source": [
        "print (hasattr(text_file, \"toDF\"))\n",
        "# False\n",
        "\n",
        "SparkSession(sc)\n",
        "print (hasattr(text_file, \"toDF\"))\n",
        "## True"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spv8xbetNt_p",
        "outputId": "cbc487d2-9832-4ece-f3f3-a80f707148db"
      },
      "source": [
        "df = sc.parallelize([(\"Tom\", \"M\", 20), (\"Jack\", \"M\", 18), (\"Marry\", \"F\", 20)])\\\n",
        "       .toDF(['name', 'sex', 'age'])\n",
        "\n",
        "df.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+---+\n",
            "| name|sex|age|\n",
            "+-----+---+---+\n",
            "|  Tom|  M| 20|\n",
            "| Jack|  M| 18|\n",
            "|Marry|  F| 20|\n",
            "+-----+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhQ1owAuO2Wj",
        "outputId": "7907b8c9-b56c-4621-b3b9-c077881eb41f"
      },
      "source": [
        "# Counts by certain column\n",
        "countsByCol = df.groupBy(\"age\").count()\n",
        "countsByCol.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-----+\n",
            "|age|count|\n",
            "+---+-----+\n",
            "| 18|    1|\n",
            "| 20|    2|\n",
            "+---+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ogbu-5QPHhr",
        "outputId": "6be55f4e-20e5-4943-98e3-f421f91f3bd6"
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "errors = df.filter(col(\"age\") > 19)\n",
        "errors.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+---+\n",
            "| name|sex|age|\n",
            "+-----+---+---+\n",
            "|  Tom|  M| 20|\n",
            "|Marry|  F| 20|\n",
            "+-----+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMWHjuRQPklM",
        "outputId": "f80adc4b-7162-48c9-994d-7c54e4221d08"
      },
      "source": [
        "# Creates a DataFrame having a single column named \"line\"\n",
        "df = text_file.map(lambda k: k.split(\"\\\\t\")).toDF([\"line\"])\n",
        "\n",
        "errors = df.filter(col(\"line\").like(\"%ERROR%\"))\n",
        "# Counts all the errors\n",
        "print (errors.count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}